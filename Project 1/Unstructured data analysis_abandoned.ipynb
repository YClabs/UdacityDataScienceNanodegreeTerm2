{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1.f Treatment for columns containing unstructured data\n",
    "\n",
    "Now we need to revisit what to do with the columns containing unstructured data. \n",
    "\n",
    "After reading around, I decided that a simple text processing model can be used to transform the unstructured text into feature vectors that could be processed by a machine learning model. In a way, I had to do basic feature extraction on those columns to extract the textual features into vectors as an input to my model.\n",
    "I referred to [this article](https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/) for extensive guidance on how to do this.\n",
    "\n",
    "First, I extract just the relevant columns and determine which columns would be useful for treatment. Referring to the data dictionary above, we had earlier determined that name, summary, space, description, neighborhood_overview, and transit contain unstructured text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'listings_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4d48e92a14a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlistings_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'space'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'description'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'neighborhood_overview'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'transit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'listings_sample' is not defined"
     ]
    }
   ],
   "source": [
    "listings_sample.loc[['name','summary', 'space', 'description', 'neighborhood_overview','transit']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sample values above, name may not contain much useful information for prediction, so to keep things simple I decided to drop it. \n",
    "\n",
    "The space, summary and description columns also appear to overlap. I have decided to keep description instead of summary as there appears to be more variation (i.e. unique values) in the description column compared to summary, so drop summary and space from the analysis.\n",
    "\n",
    "Neighborhood_overview may also contain similar information to the neighborhood column, it contains descriptions of the neighbourhood. Regardless, I decided to keep it as it may contain information that affects guests' ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract subset of columns for simple text processing from dataset\n",
    "listings_txt=listings[['description','neighborhood_overview','transit']]\n",
    "listings_txt.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.2.1.f (i) Preprocessing of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Lower case: transform all columns to lowercase\n",
    "listings_txt=listings_txt.apply(lambda x: x.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Remove punctuation\n",
    "listings_txt=listings_txt.apply(lambda x: x.str.replace('[^\\w\\s]',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. Removing stopwords and numbers from the columns\n",
    "\n",
    "We should remove all stopwords (or commonly occurring words) and numbers from the text as they do not contain useful information. I used predefined libraries of stopwords for removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Removal of Stop Words\n",
    "#add stopwords to corpus to exclude\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#create function for re-use\n",
    "def no_stop(col_no_stop,col):\n",
    "    listings_txt[col_no_stop] = listings_txt[col].apply(lambda x: ' '.join([word for word in str(x).split() if word not in (stop)]))\n",
    "\n",
    "#apply function to dataframe - note: is there are more efficient way to do this?\n",
    "for x,y in zip(list(listings_txt.columns.values+'_no_stop'),list(listings_txt.columns.values)):\n",
    "    no_stop(x,y) \n",
    "\n",
    "listings_txt.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4.Common word removal\n",
    "\n",
    "Check most frequent common words in the data and make a call to retain or to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function for re-use\n",
    "def freq(col):\n",
    "    freq = pd.Series(' '.join(listings_txt[col]).split()).value_counts()[:30]\n",
    "    return col,freq\n",
    "#check most frequent words\n",
    "[freq(col) for col in [col for col in listings_txt if col.endswith('stop')]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for all three columns, words like guest, home, house, seattle and downtown are not useful and generic, so can be dropped. \n",
    "\n",
    "For the description column, words like neighborhood, room, kitchen, bedroom, home, bed, space, apartment are all very generic and so should be dropped.\n",
    "\n",
    "For the neighborhood_overview columns, words like neighborhood is obviously generic and can be dropped. The others look okay to be kept, however.\n",
    "\n",
    "For the transit column, words like away, minutes and street are very generic, so should be dropped along with minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop selected words\n",
    "\n",
    "desc_freq = ['neighborhood','guest','home','house','seattle','downtown','room', 'kitchen', 'bathroom','bedroom', 'home', 'bed', 'space', 'apartment' ] \n",
    "neigh_freq=['guest','home','house','seattle','neighborhood','downtown','nan']\n",
    "transit_freq=['guest','home','house','seattle','away','minutes','minute','street','nan','downtown']\n",
    "\n",
    "for x,y in zip([col for col in listings_txt if col.endswith('stop')],[desc_freq,neigh_freq,transit_freq]):\n",
    "    listings_txt[x] = listings_txt[x].apply(lambda x: \" \".join(x for x in x.split() if x not in y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Rare words removal\n",
    "\n",
    "#create function for re-use\n",
    "def rare(col):\n",
    "    rare = pd.Series(' '.join(listings_txt[col]).split()).value_counts()[-10:]\n",
    "    return rare\n",
    "\n",
    "#check rare words\n",
    "[rare(col) for col in [col for col in listings_txt if col.endswith('stop')]]\n",
    "\n",
    "#add rare words to lists\n",
    "desc_rare=[]\n",
    "neigh_rare=[]\n",
    "transit_rare=[]\n",
    "\n",
    "for x,y in zip([col for col in listings_txt if col.endswith('stop')]\n",
    "               ,[desc_rare,neigh_rare,transit_rare]):\n",
    "    y=list(rare(x).index)\n",
    "    \n",
    "    \n",
    "#remove rare words\n",
    "for x,y in zip([col for col in listings_txt if col.endswith('stop')]\n",
    "               ,[desc_rare,neigh_rare,transit_rare]):\n",
    "    listings_txt[x] = listings_txt[x].apply(lambda x: \" \".join(x for x in x.split() if x not in y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "for col in [col for col in listings_txt if col.endswith('stop')]:\n",
    "    listings_txt[col] = listings_txt[col].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "listings_txt=listings_txt[[col for col in listings_txt if col.endswith('stop')]]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(listings_txt)\n",
    "train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #convert all 'nan' into np.nan\n",
    "# listings_txt=listings_txt.replace('nan', np.nan).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst1=collections.Counter(\" \".join(listings_test[\"description\"].dropna()).split()).most_common(1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
